{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Prompting** is the process of providing a partial, usually text, input to a model. As we discussed in the last chapter, models will then use their parameterized data transformations to find a probable completion or output that matches the prompt.\n",
        "\n",
        "**Prompt and AI Engineering** is the emerging developer task of designing and optimizing prompts (and associated workflows/ infra) for AI models to achieve specific goals or outcomes. It involves creating high-quality inputs that can elicit accurate and relevant responses from AI models. The next several examples will help get you up to speed on common prompt engineering strategies.\n",
        "\n",
        "```\n",
        "               +-------------------+\n",
        "               |                   |\n",
        "               |                   |  Completion\n",
        "Prompt         |       Large       |  Generated text\n",
        "--------------->     Language      +------------->\n",
        "               |       Model       |\n",
        "               |       (LLM)       |\n",
        "               |                   |\n",
        "               +-------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "o14TvGScYAqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies and imports"
      ],
      "metadata": {
        "id": "Jc-nVEbsX8bU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u9pot_Yc2FMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89590df9-fed3-419b-e9c7-bad6b3d09bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting predictionguard\n",
            "  Downloading predictionguard-2.7.0-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from predictionguard) (0.9.0)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from predictionguard) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (2024.8.30)\n",
            "Downloading predictionguard-2.7.0-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: predictionguard\n",
            "Successfully installed predictionguard-2.7.0\n"
          ]
        }
      ],
      "source": [
        "! pip install predictionguard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from predictionguard import PredictionGuard\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "rOVhsPn42JEl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pg_access_token = getpass('Enter your Prediction Guard access api key: ')\n",
        "os.environ['PREDICTIONGUARD_API_KEY'] = pg_access_token"
      ],
      "metadata": {
        "id": "l8sDezef2Me8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77933cd-4b37-483d-fb3c-63b967bdf7a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Prediction Guard access api key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = PredictionGuard()"
      ],
      "metadata": {
        "id": "IG5p4NKiBK7Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autocomplete"
      ],
      "metadata": {
        "id": "haoOqKSw2azm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because LLMs are configured/ trained to perform the task of text completion, the most basic kind of prompt that you might provide is an **autocomplete** prompt. Regardless of prompt structure, the model function will compute the probabilities of words, tokens, or characters that might follow in the sequence of words, tokens, or characters that you provided in the prompt.\n",
        "\n",
        "Depending on the desired outcome, the prompt may be a single sentence, a paragraph, or even an partial story. Additionally, the prompt may be open-ended, providing only a general topic or theme, or it may be more specific, outlining a particular scenario or plot."
      ],
      "metadata": {
        "id": "BInWcXfcYd0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Daniel Whitenack, a long forgotten wizard from the Lord of the Rings, entered into Mordor to\"}]\n",
        ")\n",
        "\n",
        "print(result['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "9EBTZ6-V2dpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0309d454-16d1-4539-df3c-ed954092a7c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieve the One Ring and destroy it in the fires of Mount Doom. However, he was not mentioned in J.R.R. Tolkien's novels or any officially licensed adaptations of the Lord of the Rings.\n",
            "\n",
            "If Daniel Whitenack were to enter Mordor, he would face immense challenges and dangers, including:\n",
            "\n",
            "1. The hostile landscape: Mordor is a harsh, barren land, with little food, water, or shelter for travelers. Its extreme conditions would make it difficult for anyone to survive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = client.chat.completions.create(\n",
        "    model=\"Neural-Chat-7B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Today I inspected the engine mounting equipment. I found a problem in one of the brackets so\"}]\n",
        ")\n",
        "\n",
        "print(result['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "13GECBS2M-Aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775b7aa0-649c-4059-b869-6c3b7cb31f8e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you need to report this issue. Please document the faulty bracket carefully, noting its location, any visible damage, or worn-out components. Next, communicate the problem to the appropriate people, such as your supervisor, the head of the maintenance department, or the individual responsible for engine mounting. Make sure to include all the relevant details you've gathered from your inspection to help expedite the solution process. If necessary, make suggestions for the replacement or repair of the damaged part.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"\"\"Complete this SQL statement:\n",
        "\n",
        "CREATE TABLE llm_queries(id SERIAL PRIMARY KEY, name TEXT NOT NULL, value REAL);\n",
        "INSERT INTO llm_queries('Daniel Whitenack', 'autocomplete')\n",
        "SELECT\"\"\"}]\n",
        ")\n",
        "\n",
        "print(result['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "RSoTsZtGNHwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e463dd3c-5681-4e14-b08a-10626eb999e6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To complete the SQL statement, you can insert the values directly into the table using the VALUES clause. Here's the updated SQL statement:\n",
            "\n",
            "CREATE TABLE llm_queries(id SERIAL PRIMARY KEY, name TEXT NOT NULL, value TEXT);\n",
            "INSERT INTO llm_queries(id, name, value) VALUES \n",
            "(DEFAULT, 'Daniel Whitenack', 'autocomplete');\n",
            "\n",
            "Explanation:\n",
            "- The first part of the statement creates a table named \"llm_queries\" with three columns: \"id\" (auto-incrementing integer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero shot, Instruction prompts"
      ],
      "metadata": {
        "id": "oZvw-mv52TL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autocomplete is a great place to start, but it is only that: a place to start. Throughout this workshop we will be putting on our prompt engineering hats to do some impressive things with generative AI. As we continue along that path, there is a general prompt structure that will pop up over and over again:\n",
        "\n",
        "```\n",
        " Prompt:\n",
        "+------------------------------------------------------------+\n",
        "|                                                            |\n",
        "|  +-------------------------------------------------------+ |\n",
        "|  | ----------------------------------------------------- | | Task Descrip./\n",
        "|  | ---------------------------------------               | | Instructions\n",
        "|  +-------------------------------------------------------+ |\n",
        "|                                                            |\n",
        "|  +-------------------------------------------------------+ | Current Input/\n",
        "|  | -------------                                         | | Context\n",
        "|  +-------------------------------------------------------+ |\n",
        "|                                                            |\n",
        "|  +----------------------------------------+                | Output\n",
        "|  | --------------------------             |                | Indicator\n",
        "|  +----------------------------------------+                |\n",
        "|                                                            |\n",
        "+------------------------------------------------------------+\n",
        "```\n",
        "\n",
        "One of the easiest ways to leverage the above prompt structure is to describe a task (e.g., sentiment analysis), provide a single piece of data as context, and then provide a single output indicator. This is called a **zero shot prompt**.\n",
        "\n",
        "Note, sometimes you can separate out the task description or instructions into the system prompt, but not always. It likely depends on how boilerplate this is between calls to the LLM."
      ],
      "metadata": {
        "id": "gmXVIVBoYsGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis"
      ],
      "metadata": {
        "id": "7yekZsMFNoxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"\"\"Respond with a sentiment label for the text included in the below user input. Use the label NEU for neutral sentiment, NEG for negative sentiment, and POS for positive sentiment. Respond only with one of these labels and no other text.\n",
        "\n",
        "Input:\n",
        "This tutorial is spectacular. I love it! So wonderful.\n",
        "\n",
        "Response: \"\"\"}]\n",
        ")\n",
        "\n",
        "print(result['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "lpBfvmQp2Ryu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53e6033-c64e-416d-a63e-99c5a56414a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering"
      ],
      "metadata": {
        "id": "5ctMfmojNz_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Read the context in the below input and respond with an answer to the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n",
        "\n",
        "Context: Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\n",
        "\n",
        "Question: How are gift cards delivered?\n",
        "\n",
        "Response: \"\"\""
      ],
      "metadata": {
        "id": "kh4jlFH1NvUP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "5e9ab2RiN7Js",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "18638dbf-c26b-4036-d929-f7a2a3a66ff5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The gift cards are delivered via US Mail.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot prompts"
      ],
      "metadata": {
        "id": "PlGYdHDy2XD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When your task is slightly more complicated or requires a few more leaps in reasoning to generate an appropriate response, you can turn to **few shot** prompting (aka **in context learning**). In few shot prompting, a small number of gold standard demonstrations are integrated into the prompt. These demonstrations serve as example (context, output) pairs for the model, which serve to tune the probable output on-the-fly to what we ideally want in the output.\n",
        "\n",
        "Although not always necessary (as seen above), few shot prompting generally produces better results than single shot prompting in terms of consistency and similarity to your ideal outputs. This does come at a cost for some models that might charge based on the number of characters or words that are input to the model API."
      ],
      "metadata": {
        "id": "Z163pVfDZJIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment"
      ],
      "metadata": {
        "id": "_DjiAH9XOBDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Classify the sentiment of the text. Here are some examples:\n",
        "\n",
        "Text: That pilot is adorable.\n",
        "Sentiment: {\"label\": \"POS\"}\n",
        "\n",
        "Text: This was an awful seat.\n",
        "Sentiment: {\"label\": \"NEG\"}\n",
        "\n",
        "Text: The flight is boring.\n",
        "Sentiment: \"\"\""
      ],
      "metadata": {
        "id": "S7mwe3BP2egC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "WcY7yVhQOJO1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8f071635-1c6e-4bc4-faf9-f921143bfbb7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"label\": \"NEG\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat, Hinglish conversation"
      ],
      "metadata": {
        "id": "EN2mxlUGOabe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI assistant in Hinglish. The assistant is helpful, creative, clever, and very friendly. The AI assistant responds in Hinglish.\n",
        "\n",
        "Human: Hello\n",
        "AI: Hello!\n",
        "\n",
        "Human: Movie ke baare mein tumhara initial opinion kya hai? Zyada depth mein gaye bina? Tumne like kiya ki nahin?\n",
        "AI: Overall, mujhe movie achchi lagi.  Usme kuh high-profile actors hain,  Tum kya kahte ho?\n",
        "\n",
        "Human: Tumhe lagta hai ki ek achche movie mein high-profile actors hone chahiye? Ya phir, ek movie ko achcha kahne ke liye high-profile actors hone chahiye?\n",
        "AI: Zaroori nahin, par help zaroor milta hai.  Ek actor jab tak apna kaam theek se nahin karta, tab tak high-profile nahin ban sakta. Tumhe kya lagta hai?\n",
        "\n",
        "Human: muje lgta hai ki yeh kewal personal pasand ke bare mai hai meri pasandida american comedy films Will Ferrol or Seth Rogan ke sath kuch bhi samil slapstick films hai muje yakin hai ki ek film thi jisme Lindsay Lohan ko bhot bada bna diya tha bhale hi usne apne surati saalo mai movies mai acting ki thi\n",
        "AI: \"\"\""
      ],
      "metadata": {
        "id": "fZLp1dhBOXOt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")['choices'][0]['message']['content']\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "tCzKXV8zOl0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b799f69-b5ee-49cb-ea29-c1952eb24c96"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abhi ka Matlab yeh hai ki actor ka talent important hai, but yeh bataate hai ki meri ek personal pasand hai American comedy films, jisme Will Ferrell aur Seth Rogen common actors hain, aur kuch slapstick films bhi. Mujhe yeh aasaani se lagey kyonki in actors aur comedy films ke baare mein mujhe bahut knowledge hai. But movie ki quality ka naya sacha matlab hai high-profile actors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Translate the following Hinglish input to English.\n",
        "\n",
        "Hinglish input:\n",
        "{hinglish}\n",
        "\n",
        "English translation:\n",
        "\"\"\".format(hinglish=output)\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "gntWOt7YOnBH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "7e686b75-38af-4bf9-f786-b70c0e3dc9b4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The meaning of \"right now\" is that an actor\\'s talent is important, but it also shows that I have a personal preference for American comedy films, in which Will Ferrell and Seth Rogen are common actors, and some slapstick films as well. It becomes easy for me because I have a lot of knowledge about these actors and comedy films. But the real meaning of movie quality is high-profile actors.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gP6yJn1IvTrb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}