{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To run any prompt through a model, we need to set a foundation for how we will **Prompting** is the process of providing a partial, usually text, input to a model. As we discussed in the last chapter, models will then use their parameterized data transformations to find a probable completion or output that matches the prompt.\n",
        "\n",
        "To run any prompt through a model, we need to set a foundation for how we will access generative AI models and perform inference. There is a huge variety in the landscape of generative AI models in terms of size, access patterns, licensing, etc. However, a common theme is the usage of LLMs through a REST API, which is either:\n",
        "- Provided by a closded third party AI service (OpenAI, Anthropic, Cohere, etc.)\n",
        "- Self-hosted in your own infrastructure or in an account you control with a platform that handles much of the infrastructure (e.g., Prediction Guard for security/privacy sensitive deployments or OpenShift AI for general k8s environments)\n",
        "- Self-hosted using a model serving framework (TGI, vLLM, etc.)\n",
        "\n",
        "We will use [Prediction Guard](https://www.predictionguard.com/) to call open access LLMs (like Llama 3.1, Mistral, deepseek, etc.) via a standardized OpenAI-like API. This will allow us to explore the full range of LLMs available. Further, it will illustrate how companies can access a wide range of models (outside of the GPT family).\n",
        "\n",
        "In order to \"prompt\" an LLM via Prediction Guard (and eventually engineer prompts), you will need to first install the Python client and supply your access token as an environment variable:"
      ],
      "metadata": {
        "id": "tx0f1rKRWqS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependences, imports"
      ],
      "metadata": {
        "id": "ZGe8RF_LzKjK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pI0jTm47xNj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4532de7-cdf7-4137-91b8-a5281004c672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting predictionguard\n",
            "  Downloading predictionguard-2.7.0-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from predictionguard) (0.9.0)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from predictionguard) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (2024.8.30)\n",
            "Downloading predictionguard-2.7.0-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: predictionguard\n",
            "Successfully installed predictionguard-2.7.0\n"
          ]
        }
      ],
      "source": [
        "! pip install predictionguard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from predictionguard import PredictionGuard\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "Wg7xvnBhxb38"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pg_access_token = getpass('Enter your Prediction Guard access api key: ')\n",
        "os.environ['PREDICTIONGUARD_API_KEY'] = pg_access_token"
      ],
      "metadata": {
        "id": "K_cUA6tClxcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e003a85-53c2-4ce4-c00d-225e555eb33c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Prediction Guard access api key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = PredictionGuard()"
      ],
      "metadata": {
        "id": "30aADVE3_nvS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List available models"
      ],
      "metadata": {
        "id": "eutZBE8vtLC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find out more about the models available via the Prediction Guard API [in the docs](https://docs.predictionguard.com/models)."
      ],
      "metadata": {
        "id": "obLO0rEGtPTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.chat.completions.list_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRTwUV7CApWi",
        "outputId": "6fc1beb8-b27f-497b-d57d-2cf1567c5201"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['deepseek-coder-6.7b-instruct',\n",
              " 'Hermes-2-Pro-Llama-3-8B',\n",
              " 'Hermes-2-Pro-Mistral-7B',\n",
              " 'Hermes-3-Llama-3.1-70B',\n",
              " 'Hermes-3-Llama-3.1-8B',\n",
              " 'llava-1.5-7b-hf',\n",
              " 'llava-v1.6-mistral-7b-hf',\n",
              " 'neural-chat-7b-v3-3']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.embeddings.list_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxWsF8R1AppA",
        "outputId": "3a539e68-23cf-4256-9d6a-bc550b8ed690"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bridgetower-large-itm-mlm-itc', 'multilingual-e5-large-instruct']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate some text with an LLM\n",
        "\n"
      ],
      "metadata": {
        "id": "fr7dHK-VyW2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    prompt=\"Some of the best advice I can give is \"\n",
        ")\n",
        "\n",
        "print(json.dumps(\n",
        "    response,\n",
        "    sort_keys=True,\n",
        "    indent=4,\n",
        "    separators=(',', ': ')\n",
        "))"
      ],
      "metadata": {
        "id": "9xw7U9qDzPKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb3bf72-c2df-47b2-a258-8b85b6dac422"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"choices\": [\n",
            "        {\n",
            "            \"index\": 0,\n",
            "            \"text\": \"1) don\\u2019t put all of your eggs in one basket and 2) make sure you have a plan. I\\u2019ve learned this through trial and error. With investing, diversification is key. Putting your money into a single stock or sector could leave you exposed to a lot of risk. If that stock or sector tanks, you could lose all of your investment. But by investing in a variety of stocks and sectors, you can spread out that risk. You could still lose money, of course\"\n",
            "        }\n",
            "    ],\n",
            "    \"created\": 1732278275,\n",
            "    \"id\": \"cmpl-a34f8090-f1d8-4ed6-817d-d4c54b6d784f\",\n",
            "    \"model\": \"Hermes-3-Llama-3.1-8B\",\n",
            "    \"object\": \"text_completion\",\n",
            "    \"usage\": {\n",
            "        \"completion_tokens\": 0,\n",
            "        \"prompt_tokens\": 0,\n",
            "        \"total_tokens\": 0\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-jeL4vF3xSui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}