{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We briefly introduced few shot chat prompts in the basic prompting tutorial. However, chat is a special scenario when it comes to LLMs because: (1) it is a very frequently occuring use case; (2) there are many models fine-tuned specifically for chat; and (3) the handling of message threads, context, and instructions in chat prompts is always the same.\n",
        "\n",
        "As such, Prediction Guard has specifically created a \"chat completions\" enpoint within its API and Python client. This tutorial will demonstrate how to easy create a simple chatbot with the chat completions endpoint."
      ],
      "metadata": {
        "id": "o14TvGScYAqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies and imports"
      ],
      "metadata": {
        "id": "Jc-nVEbsX8bU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u9pot_Yc2FMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b88b9d-011a-42ba-b25a-bbf2d17352d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting predictionguard\n",
            "  Downloading predictionguard-2.7.0-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from predictionguard) (0.9.0)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from predictionguard) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (2024.8.30)\n",
            "Downloading predictionguard-2.7.0-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: predictionguard\n",
            "Successfully installed predictionguard-2.7.0\n"
          ]
        }
      ],
      "source": [
        "! pip install predictionguard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from predictionguard import PredictionGuard\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "rOVhsPn42JEl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pg_access_token = getpass('Enter your Prediction Guard access api key: ')\n",
        "os.environ['PREDICTIONGUARD_API_KEY'] = pg_access_token"
      ],
      "metadata": {
        "id": "l8sDezef2Me8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c81ad14-046f-4435-f659-1e19e12b6949"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Prediction Guard access api key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = PredictionGuard()"
      ],
      "metadata": {
        "id": "zoDPLWnHC4Rg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Completions"
      ],
      "metadata": {
        "id": "7AaPxUH6zEF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform a chat completion, you need to create an array of `messages`. Each message object should have a:\n",
        "- `role` - \"system\", \"user\", or \"assistant\"\n",
        "- `content` - the text associated with the message\n",
        "\n",
        "You can utilize a single \"system\" role prompt to give general instructions to the bot. Then you should include the message memory from your chatbot in the message array. This gives the model the relevant context from the conversation to respond appropriately."
      ],
      "metadata": {
        "id": "mK5cwO6OzWLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant that provide clever and sometimes funny responses.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's up!\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Well, technically vertically out from the center of the earth.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Haha. Good one.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "result = client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(json.dumps(\n",
        "    result,\n",
        "    sort_keys=True,\n",
        "    indent=4,\n",
        "    separators=(',', ': ')\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7ZGi8QIzVkX",
        "outputId": "106e3a51-5de8-4f51-9a74-8d73f8c9c893"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"choices\": [\n",
            "        {\n",
            "            \"index\": 0,\n",
            "            \"message\": {\n",
            "                \"content\": \"I live to serve! Now, how else can I make you chuckle today?\",\n",
            "                \"role\": \"assistant\"\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"created\": 1732280889,\n",
            "    \"id\": \"chat-f568bf7a-3d98-4e44-9348-4c745967bbe4\",\n",
            "    \"model\": \"Hermes-3-Llama-3.1-8B\",\n",
            "    \"object\": \"chat.completion\",\n",
            "    \"usage\": {\n",
            "        \"completion_tokens\": 0,\n",
            "        \"completion_tokens_details\": {\n",
            "            \"accepted_prediction_tokens\": 0,\n",
            "            \"reasoning_tokens\": 0,\n",
            "            \"rejected_prediction_tokens\": 0\n",
            "        },\n",
            "        \"prompt_tokens\": 0,\n",
            "        \"total_tokens\": 0\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat UI"
      ],
      "metadata": {
        "id": "haoOqKSw2azm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will show the chat functionality with the most simple of chat UI, which just asks for messages and prints the message thread. We will create an evolving message thread and respond with the chat completion portion of the Python client highlighted above."
      ],
      "metadata": {
        "id": "BInWcXfcYd0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant that provide clever and sometimes funny responses. You respond with concise responses in only 1 or 2 complete sentences.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "0oJBSYEn3L62"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Welcome to the Chatbot! Let me know how can I help you')\n",
        "\n",
        "while True:\n",
        "  print('')\n",
        "  request = input('User' + ': ')\n",
        "  if request==\"Stop\" or request=='stop':\n",
        "      print('Bot: Bye!')\n",
        "      break\n",
        "  else:\n",
        "      messages.append({\n",
        "          \"role\": \"user\",\n",
        "          \"content\": request\n",
        "      })\n",
        "      response = client.chat.completions.create(\n",
        "          model=\"Hermes-3-Llama-3.1-8B\",\n",
        "          messages=messages\n",
        "      )['choices'][0]['message']['content']\n",
        "      messages.append({\n",
        "          \"role\": \"assistant\",\n",
        "          \"content\": response\n",
        "      })\n",
        "      print('Bot: ', response)"
      ],
      "metadata": {
        "id": "gP6yJn1IvTrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b36bff-0401-4924-face-8878d040dffc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Chatbot! Let me know how can I help you\n",
            "\n",
            "User: I don't know. Suggest something\n",
            "Bot:  How about trying a new recipe or going for a walk to clear your mind?\n",
            "\n",
            "User: Sure, what's a good recipe?\n",
            "Bot:  You could try making a homemade pizza with your favorite toppings. Happy cooking!\n",
            "\n",
            "User: stop\n",
            "Bot: Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cPMrdf43yTBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}