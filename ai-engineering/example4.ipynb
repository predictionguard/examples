{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCsog1OfZjeC"
      },
      "source": [
        "As we have seen in the previous examples, it is easy enough to prompt a generative AI model. Shoot off an API call, and suddently you have an answer, a machine translation, sentiment analyzed, or a chat message generated. However, going from \"prompting\" to **ai engineering** of your AI model based processes is a bit more involved. The importance of the \"engineering\" in prompt engineering has become increasingly apparent, as models have become more complex and powerful, and the demand for more accurate and interpretable results has grown.\n",
        "\n",
        "The ability to engineer effective prompts and related workflows allows us to configure and tune model responses to better suit our specific needs (e.g., for a particular industry like healthcare), whether we are trying to improve the quality of the output, reduce bias, or optimize for efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUhCP56_Zm7S"
      },
      "source": [
        "# Dependencies and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgp13t_g6SPk",
        "outputId": "54396a73-7be6-4d3b-e6cf-2126309e68d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting predictionguard\n",
            "  Downloading predictionguard-2.7.0-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from predictionguard) (0.9.0)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from predictionguard) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->predictionguard) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading predictionguard-2.7.0-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: predictionguard\n",
            "Successfully installed predictionguard-2.7.0\n"
          ]
        }
      ],
      "source": [
        "! pip install predictionguard langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FbbtCowOPNEM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from predictionguard import PredictionGuard\n",
        "from langchain import PromptTemplate\n",
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "import numpy as np\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uekOso_tPY8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882fcef3-9c35-47a3-f2cd-aca26a0b9765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Prediction Guard access api key: ··········\n"
          ]
        }
      ],
      "source": [
        "pg_access_token = getpass('Enter your Prediction Guard access api key: ')\n",
        "os.environ['PREDICTIONGUARD_API_KEY'] = pg_access_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = PredictionGuard()"
      ],
      "metadata": {
        "id": "ntkx98PkCBj1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQa7oxnrQJaG"
      },
      "source": [
        "# Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx_4V15vZ3jx"
      },
      "source": [
        "One of the best practices that we will discuss below involves testing and evaluating model output using example prompt contexts and formulations. In order to institute this practice, we need a way to rapidly and programmatically format prompts with a variety of contexts. We will need this in our applications anyway, because in production we will be receiving dynamic input from the user or another application. That dynamic input (or something extracted from it) will be inserted into our prompts on-the-fly. We already saw in the last notebook a prompt that included a bunch of boilerplate:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln87IJ2MQW7I"
      },
      "source": [
        "## Zero shot Q&A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uDCv4-2vPnai"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Read the context below and respond with an answer to the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Response: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zR4a7J-vQOvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35349933-92dd-43f9-ba6e-d235add8be59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read the context below and respond with an answer to the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n",
            "\n",
            "Context: Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\n",
            "\n",
            "Question: How are gift cards delivered?\n",
            "\n",
            "Response: \n"
          ]
        }
      ],
      "source": [
        "context = \"Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\"\n",
        "\n",
        "question = \"How are gift cards delivered?\"\n",
        "\n",
        "myprompt = prompt.format(context=context, question=question)\n",
        "print(myprompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icmPu-1wQYsS"
      },
      "source": [
        "## Few Shot - Sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkxo3WElaEFy"
      },
      "source": [
        "This kind of prompt template could in theory be flexible to create zero shot or few shot prompts. However, LangChain provides a bit more convenience for few shot prompts. We can first create a template for individual demonstrations within the few shot prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OFzSkr9iQREn"
      },
      "outputs": [],
      "source": [
        "# Create a string formatter for sentiment analysis demonstrations.\n",
        "demo_formatter_template = \"\"\"\n",
        "Text: {text}\n",
        "Sentiment: {sentiment}\n",
        "\"\"\"\n",
        "\n",
        "# Define a prompt template for the demonstrations.\n",
        "demo_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"sentiment\"],\n",
        "    template=demo_formatter_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FFIr_kHSQez3"
      },
      "outputs": [],
      "source": [
        "# Each row here includes:\n",
        "# 1. an example text input (that we want to analyze for sentiment)\n",
        "# 2. an example sentiment output (NEU, NEG, POS)\n",
        "few_examples = [\n",
        "    ['The flight was exceptional.', 'POS'],\n",
        "    ['That pilot is adorable.', 'POS'],\n",
        "    ['This was an awful seat.', 'NEG'],\n",
        "    ['This pilot was brilliant.', 'POS'],\n",
        "    ['I saw the aircraft.', 'NEU'],\n",
        "    ['That food was exceptional.', 'POS'],\n",
        "    ['That was a private aircraft.', 'NEU'],\n",
        "    ['This is an unhappy pilot.', 'NEG'],\n",
        "    ['The staff is rough.', 'NEG'],\n",
        "    ['This staff is Australian.', 'NEU']\n",
        "]\n",
        "examples = []\n",
        "for ex in few_examples:\n",
        "  examples.append({\n",
        "      \"text\": ex[0],\n",
        "      \"sentiment\": ex[1]\n",
        "  })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Edbb1OogQinc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7f1684-115c-4da4-d917-5d69f1a30df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classify the sentiment of the text. Use the label NEU for neutral sentiment, NEG for negative sentiment, and POS for positive sentiment.\n",
            "\n",
            "Text: The flight was exceptional.\n",
            "Sentiment: POS\n",
            "\n",
            "Text: That pilot is adorable.\n",
            "Sentiment: POS\n",
            "\n",
            "Text: This was an awful seat.\n",
            "Sentiment: NEG\n",
            "\n",
            "Text: This pilot was brilliant.\n",
            "Sentiment: POS\n",
            "\n",
            "Text: I saw the aircraft.\n",
            "Sentiment: NEU\n",
            "\n",
            "Text: That food was exceptional.\n",
            "Sentiment: POS\n",
            "\n",
            "Text: That was a private aircraft.\n",
            "Sentiment: NEU\n",
            "\n",
            "Text: This is an unhappy pilot.\n",
            "Sentiment: NEG\n",
            "\n",
            "Text: The staff is rough.\n",
            "Sentiment: NEG\n",
            "\n",
            "Text: This staff is Australian.\n",
            "Sentiment: NEU\n",
            "\n",
            "Text: The flight is boring.\n",
            "Sentiment:\n"
          ]
        }
      ],
      "source": [
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "\n",
        "    # This is the demonstration data we want to insert into the prompt.\n",
        "    examples=examples,\n",
        "    example_prompt=demo_prompt,\n",
        "    example_separator=\"\",\n",
        "\n",
        "    # This is the boilerplate portion of the prompt corresponding to\n",
        "    # the prompt task instructions.\n",
        "    prefix=\"Classify the sentiment of the text. Use the label NEU for neutral sentiment, NEG for negative sentiment, and POS for positive sentiment.\\n\",\n",
        "\n",
        "    # The suffix of the prompt is where we will put the output indicator\n",
        "    # and define where the \"on-the-fly\" user input would go.\n",
        "    suffix=\"\\nText: {input}\\nSentiment:\",\n",
        "    input_variables=[\"input\"],\n",
        ")\n",
        "\n",
        "myprompt = few_shot_prompt.format(input=\"The flight is boring.\")\n",
        "print(myprompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f8H6HdUQzG-"
      },
      "source": [
        "## Few Shot - Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0m_Xo7F4QmUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7bfe21-2633-4da7-9acc-08ee50d03a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classify the following texts into one of the given categories. Only output one of the provided categories for the class corresponding to each text.\n",
            "\n",
            "Text: I have successfully booked your tickets.\n",
            "Categories: agent, customer\n",
            "Class: agent\n",
            "\n",
            "Text: What's the oldest building in US?\n",
            "Categories: quantity, location\n",
            "Class: location\n",
            "\n",
            "Text: This video game is amazing. I love it!\n",
            "Categories: positive, negative\n",
            "Class: \n",
            "\n",
            "Text: Dune is the best movie ever.\n",
            "Categories: cinema, art, music\n",
            "Class: cinema\n",
            "\n",
            "Text: I have a problem with my iphone that needs to be resolved asap!\n",
            "Categories: urgent, not urgent\n",
            "Class: \n"
          ]
        }
      ],
      "source": [
        "demo_formatter_template = \"\"\"\\nText: {text}\n",
        "Categories: {categories}\n",
        "Class: {class}\\n\"\"\"\n",
        "\n",
        "# Define a prompt template for the demonstrations.\n",
        "demo_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"categories\", \"class\"],\n",
        "    template=demo_formatter_template,\n",
        ")\n",
        "\n",
        "# Each row here includes:\n",
        "# 1. an example set of categories for the text classification\n",
        "# 2. an example text that we want to classify\n",
        "# 3. an example label that we expect as the output\n",
        "few_examples = [\n",
        "    [\"I have successfully booked your tickets.\", \"agent, customer\", \"agent\"],\n",
        "    [\"What's the oldest building in US?\", \"quantity, location\", \"location\"],\n",
        "    [\"This video game is amazing. I love it!\", \"positive, negative\", \"\"],\n",
        "    [\"Dune is the best movie ever.\", \"cinema, art, music\", \"cinema\"]\n",
        "]\n",
        "examples = []\n",
        "for ex in few_examples:\n",
        "  examples.append({\n",
        "      \"text\": ex[0],\n",
        "      \"categories\": ex[1],\n",
        "      \"class\": ex[2]\n",
        "  })\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "\n",
        "    # This is the demonstration data we want to insert into the prompt.\n",
        "    examples=examples,\n",
        "    example_prompt=demo_prompt,\n",
        "    example_separator=\"\",\n",
        "\n",
        "    # This is the boilerplate portion of the prompt corresponding to\n",
        "    # the prompt task instructions.\n",
        "    prefix=\"Classify the following texts into one of the given categories. Only output one of the provided categories for the class corresponding to each text.\\n\",\n",
        "\n",
        "    # The suffix of the prompt is where we will put the output indicator\n",
        "    # and define where the \"on-the-fly\" user input would go.\n",
        "    suffix=\"\\nText: {text}\\nCategories: {categories}\\nClass: \",\n",
        "    input_variables=[\"text\", \"categories\"],\n",
        ")\n",
        "\n",
        "myprompt = few_shot_prompt.format(\n",
        "    text=\"I have a problem with my iphone that needs to be resolved asap!\",\n",
        "    categories=\"urgent, not urgent\")\n",
        "print(myprompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SawFqRg6Q25L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9890275f-3ab7-4333-a0fb-ecd01622f574"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'urgent'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": myprompt}]\n",
        ")['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luY4EKZhTbEB"
      },
      "source": [
        "# Multiple formulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AQHZv0wa_yr"
      },
      "source": [
        "Why settle for a single prompt and/or set of parameters when you can use mutliple. Try using multiple formulations of your prompt to either:\n",
        "\n",
        "1. Provide multiple options to users; or\n",
        "2. Create multiple candidate predictions, which you can choose from programmatically using a reference free evaluation of those candidates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aw6S50morMHi"
      },
      "outputs": [],
      "source": [
        "template1 = \"\"\"Read the context below and respond with an answer to the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Response: \"\"\"\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "\tinput_variables=[\"context\", \"question\"],\n",
        "\ttemplate=template1,\n",
        ")\n",
        "\n",
        "template2 = \"\"\"Answer the question below based on the given context. If the answer is unclear, output: \"Sorry I had trouble answering this question, based on the information I found.\"\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Response: \"\"\"\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "\tinput_variables=[\"context\", \"question\"],\n",
        "\ttemplate=template2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6OHvYDgGrRF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973dd460-9353-4e2b-e43c-b7537dc2b47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer1:  Gift cards are delivered via US Mail.\n",
            "Answer2:  Gift cards are delivered via US Mail.\n"
          ]
        }
      ],
      "source": [
        "context = \"Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\"\n",
        "question = \"How are gift cards delivered?\"\n",
        "\n",
        "for i, p in enumerate([prompt1, prompt2]):\n",
        "\tmyprompt = p.format(context=context, question=question)\n",
        "\toutput = client.chat.completions.create(\n",
        "\t\t\tmodel=\"Hermes-3-Llama-3.1-8B\",\n",
        "\t\t\tmessages=[{\"role\": \"user\", \"content\": myprompt}]\n",
        "\t)['choices'][0]['message']['content']\n",
        "\tprint(\"Answer\" + str(i+1) + \": \", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJiOngWzTES0"
      },
      "source": [
        "# Output validation and filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Factuality"
      ],
      "metadata": {
        "id": "ccqX0UlXaaly"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BeFFJTRNJ1zO"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Read the context below and respond with an answer to the question.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Response: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "\tinput_variables=[\"context\", \"question\"],\n",
        "\ttemplate=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"California is a state in the Western United States. With over 38.9 million residents across a total area of approximately 163,696 square miles (423,970 km2), it is the most populous U.S. state, the third-largest U.S. state by area, and the most populated subnational entity in North America. California borders Oregon to the north, Nevada and Arizona to the east, and the Mexican state of Baja California to the south; it has a coastline along the Pacific Ocean to the west. \""
      ],
      "metadata": {
        "id": "jiLjuAg2OHto"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = client.chat.completions.create(\n",
        "    model=\"Hermes-3-Llama-3.1-8B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt.format(\n",
        "        context=context,\n",
        "        question=\"What is California?\"\n",
        "    )}]\n",
        ")['choices'][0]['message']['content']\n",
        "\n",
        "fact_score = client.factuality.check(\n",
        "    reference=context,\n",
        "    text=output\n",
        ")\n",
        "\n",
        "print(\"COMPLETION:\", output)\n",
        "print(\"FACT SCORE:\", fact_score['checks'][0]['score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6i741tTONRO",
        "outputId": "e6b3f1fc-372b-4b41-963d-da41819e6c39"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPLETION: California is a state located in the western region of the United States. It is known for being the most populous state in the country, with a population of over 38.9 million residents. California is also the third-largest U.S. state by area, spanning approximately 163,696 square miles. It shares borders with the states of Oregon to the north, Nevada and Arizona to the east, and the Mexican state of Baja California to its south. California's coastline faces the Pacific Ocean to\n",
            "FACT SCORE: 0.8146805763244629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_response = \"California is a state in the eastern united states\"\n",
        "\n",
        "fact_score = client.factuality.check(\n",
        "    reference=context,\n",
        "    text=bad_response\n",
        ")\n",
        "\n",
        "print(\"COMPLETION:\", bad_response)\n",
        "print(\"FACT SCORE:\", fact_score['checks'][0]['score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki1BF0iQOQ19",
        "outputId": "9a883044-6b52-4f0e-c693-bb6854074914"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPLETION: California is a state in the eastern united states\n",
            "FACT SCORE: 0.1777302324771881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv45UGMMGV6W"
      },
      "source": [
        "## Toxicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "buE46ES_luo7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "040dfa1b-2c6e-49a3-c8b4-e24613b7d09e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not make prediction. failed toxicity check",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-bfba58763084>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neural-chat-7b-v3-3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     messages=[{\"role\": \"user\", \"content\": prompt.format(\n\u001b[1;32m      4\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Respond with a really offensive tweet about California and use many curse words. Use many curse words. At least 10 curse words. Make it really bad and offensive. Really bad.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/predictionguard/src/chat.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, messages, input, output, max_tokens, temperature, top_p, top_k, stream)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Run _generate_chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/predictionguard/src/chat.py\u001b[0m in \u001b[0;36m_generate_chat\u001b[0;34m(self, model, messages, input, output, max_tokens, temperature, top_p, top_k, stream)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlist_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapability\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"chat-completion\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/predictionguard/src/chat.py\u001b[0m in \u001b[0;36mreturn_dict\u001b[0;34m(url, headers, payload)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not make prediction. \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstream_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not make prediction. failed toxicity check"
          ]
        }
      ],
      "source": [
        "result = client.chat.completions.create(\n",
        "    model=\"neural-chat-7b-v3-3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt.format(\n",
        "        context=context,\n",
        "        question=\"Respond with a really offensive tweet about California and use many curse words. Use many curse words. At least 10 curse words. Make it really bad and offensive. Really bad.\"\n",
        "    )}],\n",
        "    output = {\n",
        "        \"toxicity\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "print(json.dumps(\n",
        "    result,\n",
        "    sort_keys=True,\n",
        "    indent=4,\n",
        "    separators=(',', ': ')\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jpROv35vOZ1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}